{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "failing-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import *\n",
    "#Connect to the cluster\n",
    "# New API\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.207:7077\") \\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",2)\\\n",
    "        .appName(\"pa_test\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-habitat",
   "metadata": {},
   "source": [
    "# Research if there is a correlation between comment length - positive/negative words - score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-niagara",
   "metadata": {},
   "source": [
    "## Load sentiment files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "oriental-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlines = sc.textFile(\"hdfs://192.168.2.207:9000/user/ubuntu/negative-words.txt\")\n",
    "plines = sc.textFile(\"hdfs://192.168.2.207:9000/user/ubuntu/positive-words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unable-indie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced',\n",
       " '2-faces',\n",
       " 'abnormal',\n",
       " 'abolish',\n",
       " 'abominable',\n",
       " 'abominably',\n",
       " 'abominate',\n",
       " 'abomination',\n",
       " 'abort',\n",
       " 'aborted']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlines.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-investigator",
   "metadata": {},
   "source": [
    "## Prepare negative and positive word data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convenient-newport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0x7f413051b128>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compile_regexp(word_list):\n",
    "    re_string = \"[\\s\\W](\"\n",
    "    for word in word_list:\n",
    "        re_string += (re.escape(word) + \"|\")\n",
    "    re_string = re_string[0:-1] + \")[\\s\\W]\"\n",
    "    return re.compile(re_string, re.IGNORECASE)\n",
    "\n",
    "negative = compile_regexp(nlines.collect())\n",
    "sc.broadcast(negative)\n",
    "positive = compile_regexp(plines.collect())\n",
    "sc.broadcast(positive)\n",
    "\n",
    "\n",
    "#df = spark.read.json(\"hdfs://192.168.2.207:9000/user/ubuntu/RC_2010-*\")\n",
    "#data_clean = df.select(\"subreddit\", \"body\", \"score\", \"controversiality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rural-tours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_sre.SRE_Pattern'>\n"
     ]
    }
   ],
   "source": [
    "print(type(negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-story",
   "metadata": {},
   "source": [
    "## Load Reddit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "musical-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.json(\"hdfs://192.168.2.207:9000/user/ubuntu/sample_data.json\")\n",
    "df_correlate_score_length = df_test.select(\"subreddit\", \"body\", \"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "quantitative-present",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "working-founder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----+\n",
      "|         subreddit|                body|score|\n",
      "+------------------+--------------------+-----+\n",
      "|          sandiego|            A quarry|    3|\n",
      "|              RWBY|[Salutations! I'm...|    3|\n",
      "|          baseball|I got into baseba...|    2|\n",
      "|         2007scape|        FUCKING TORY|   18|\n",
      "| mildlyinteresting|I see a water dra...|    1|\n",
      "|            Cubers|Wait. The Michiga...|    1|\n",
      "|         teenagers|              ye fam|    2|\n",
      "|       4chan4trump|143417804| &gt; U...|    1|\n",
      "|               CFB|That is some chic...|    2|\n",
      "|        rugbyunion|Does he even know...|    1|\n",
      "|               CFB|            Tequila.|    2|\n",
      "|         EchoArena|your heart beats ...|    1|\n",
      "|               HFY|&gt; Subscribe: /...|    1|\n",
      "|        The_Donald|you're really ign...|    2|\n",
      "|        CrazyIdeas|lets see how deep...|    1|\n",
      "|             NBA2k|You are arguing t...|    2|\n",
      "|           opiates|I'm thinking abou...|    2|\n",
      "|ImagesOfNewZealand|[Original post](h...|    1|\n",
      "|          totalwar|I think that's a ...|    3|\n",
      "|      tvcrossovers|Harp absolutelly....|    1|\n",
      "+------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_correlate_score_length.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "brilliant-little",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_correlate_score_length.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-deviation",
   "metadata": {},
   "source": [
    "## Filter to exclude empty/missing comment or score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "general-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import org.apache.spark.sql.SparkSession\n",
    "df_correlate_score_length = df_correlate_score_length.withColumn(\"score\", df_correlate_score_length['score'].cast('int'))\n",
    "df_clean = df_correlate_score_length.filter( (df_correlate_score_length.body != \"[deleted]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-attachment",
   "metadata": {},
   "source": [
    "## Add 3 new Cols, # words, #of negative words, # of positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "vital-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_negative(comment):\n",
    "    return len(negative.findall(comment))\n",
    "\n",
    "def count_positive(comment):\n",
    "    return len(positive.findall(comment))\n",
    "\n",
    "def count_words(comment):\n",
    "    return len(comment.split())\n",
    "\n",
    "udf_count_negative = udf(count_negative, IntegerType())\n",
    "udf_count_positive = udf(count_positive, IntegerType())\n",
    "udf_count_words = udf(count_words, IntegerType())\n",
    "\n",
    "\n",
    "word_count = df_clean.withColumn('words', udf_count_words('body'))\n",
    "negative_add = word_count.withColumn('negativeWords', udf_count_negative('body'))\n",
    "neg_pos_df = negative_add.withColumn('positiveWords', udf_count_positive('body'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_negative(comment, wc):\n",
    "    return len(negative.findall(comment))/wc\n",
    "\n",
    "def match_positive(comment, wc):\n",
    "    return len(positive.findall(comment))/wc\n",
    "\n",
    "def count_words(comment):\n",
    "    return len(comment.split())\n",
    "\n",
    "udf_match_negative = udf(match_negative, DoubleType())\n",
    "udf_match_positive = udf(match_positive, DoubleType())\n",
    "udf_count_words = udf(count_words, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-chicago",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-campbell",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ruled-thickness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----+-----+-------------+-------------+\n",
      "|         subreddit|                body|score|words|negativeWords|positiveWords|\n",
      "+------------------+--------------------+-----+-----+-------------+-------------+\n",
      "|          sandiego|            A quarry|    3|    2|            0|            0|\n",
      "|              RWBY|[Salutations! I'm...|    3|    7|            0|            0|\n",
      "|          baseball|I got into baseba...|    2|   27|            1|            0|\n",
      "|         2007scape|        FUCKING TORY|   18|    2|            0|            0|\n",
      "| mildlyinteresting|I see a water dra...|    1|    5|            0|            0|\n",
      "|            Cubers|Wait. The Michiga...|    1|   20|            0|            1|\n",
      "|         teenagers|              ye fam|    2|    2|            0|            0|\n",
      "|       4chan4trump|143417804| &gt; U...|    1|   29|            0|            0|\n",
      "|               CFB|That is some chic...|    2|    9|            1|            0|\n",
      "|        rugbyunion|Does he even know...|    1|    6|            0|            0|\n",
      "|               CFB|            Tequila.|    2|    1|            0|            0|\n",
      "|         EchoArena|your heart beats ...|    1|   55|            2|            2|\n",
      "|               HFY|&gt; Subscribe: /...|    1|    3|            0|            0|\n",
      "|        The_Donald|you're really ign...|    2|    9|            2|            1|\n",
      "|        CrazyIdeas|lets see how deep...|    1|    8|            0|            0|\n",
      "|             NBA2k|You are arguing t...|    2|   33|            1|            0|\n",
      "|           opiates|I'm thinking abou...|    2|   12|            0|            1|\n",
      "|ImagesOfNewZealand|[Original post](h...|    1|   57|            2|            0|\n",
      "|          totalwar|I think that's a ...|    3|  192|           11|            6|\n",
      "|      tvcrossovers|Harp absolutelly....|    1|   49|            3|            3|\n",
      "+------------------+--------------------+-----+-----+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neg_pos_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-services",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By subreddit\n",
    "\n",
    "#find \"happiest\" subreddit of the day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-antique",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-reasoning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accessory-chapel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(neg_pos_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-classics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "literary-color",
   "metadata": {},
   "source": [
    "## Research for correlation between score and #positive or #negative words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-teaching",
   "metadata": {},
   "source": [
    "* Assume normal distribution in length of comments\n",
    "* This way we can create intervals to split the data and search for patterns if any\n",
    "* For example if a comment falls near the average length of comments is it more likely to contain more negative or positive words?\n",
    "\n",
    "## Another approach\n",
    "\n",
    "* Assume normal dist in score among all posts\n",
    "* Create intervals \n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "leading-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neg_pos_df.printSchema()\n",
    "#neg_pos_df.select('score', 'words', 'negativeWords','positiveWords').show()\n",
    "df_cl1 = neg_pos_df.select(\"score\", \"words\", \"negativeWords\", \"positiveWords\", \"subreddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "smart-bench",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------+-------------+\n",
      "|score|words|negativeWords|positiveWords|\n",
      "+-----+-----+-------------+-------------+\n",
      "|    2|   12|            0|            1|\n",
      "|    1|   11|            1|            0|\n",
      "|    1|   25|            0|            1|\n",
      "|    2|   45|            1|            2|\n",
      "|    2|    5|            1|            0|\n",
      "|    0|   96|            6|            4|\n",
      "|   28|   16|            1|            0|\n",
      "|    0|    4|            0|            0|\n",
      "|    0|   21|            1|            2|\n",
      "|    1|  345|           11|           13|\n",
      "|    2|   87|            6|            2|\n",
      "|    3|   42|            1|            0|\n",
      "|    1|   88|            0|            4|\n",
      "|    2|   16|            0|            1|\n",
      "|   21|   20|            0|            1|\n",
      "|    1|  149|            1|            6|\n",
      "|    8|    1|            0|            0|\n",
      "|    4|   23|            1|            1|\n",
      "|    1|   25|            0|            0|\n",
      "|    1|    9|            0|            1|\n",
      "+-----+-----+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cl1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "billion-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = df_cl1.agg({\"words\": \"max\"}).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "several-hawaii",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1823"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-foundation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
